// This file adds the SSE streaming endpoint to ai.controller.ts
// Insert this method after the existing chat() method

/**
 * Streaming chat endpoint using Server-Sent Events
 * 
 * POST /api/ai/chat-stream
 * 
 * Returns real-time AI responses as they're generated
 */
@Post('chat-stream')
@HttpCode(HttpStatus.OK)
async chatStream(
    @Body() dto: ChatRequest,
    @Res() res: Response,
) {
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');
    res.setHeader('X-Accel-Buffering', 'no'); // Disable nginx buffering

    try {
        const { message, messages = [], context } = dto;
        
        // Build enhanced context
        const enhancedContext = context || 'No additional context provided';
        
        // Convert to conversation history
        const conversationHistory = messages.length > 0 
            ? messages 
            : [{ role: 'user', content: message }];

        // Get streaming response
        const stream = this.streaming.chatStreamEnhanced(enhancedContext, conversationHistory);

        for await (const chunk of stream) {
            if (chunk.error) {
                res.write(`data: ${JSON.stringify({ error: chunk.error })}\n\n`);
                break;
            }
            
            res.write(`data: ${JSON.stringify(chunk)}\n\n`);
            
            if (chunk.done) {
                break;
            }
        }

        res.end();
    } catch (error) {
        res.write(`data: ${JSON.stringify({ error: error.message, done: true })}\n\n`);
        res.end();
    }
}
